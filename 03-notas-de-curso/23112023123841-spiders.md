---
aliases:
  - Spider é uma classe python responsável pela extração dos dados
tags:
  - python_scprapy
---
%%
cData:: `=dateformat(this.file.ctime, "dd-MM-yyyy")`
mData:: `=dateformat(this.file.mtime, "dd-MM-yyyy")`
%%

___
# Spider é uma classe python responsável pela extração dos dados

Spider é uma classe python responsável pela extração dos dados.

Em um mesmo projeto, é possível ter várias Spiders raspando o mesmo site ou sites diferentes e armazenando os dados em locais diferentes.

## Características fundamentais das Spiders:

- **Assíncrono**: O Scrapy é construído usando a estrutura `Twisted`,  uma biblioteca de programação assíncrona para Python.  Uma solicitação a um site, não bloqueia o fluxo do programa. Assim que uma resposta é bem sucedida, o método `parse` é acionado usando uma função *callback* definida para processar essa resposta. Esse procedimento é responsabilidade da classe `Request()`.
- **Nome Único:** Cada Spider deve ter um nome exclusivo para que o Scrapy possa identificá-lo.
- **Solicitações Iniciais:**  São os pontos iniciais da Spider. Usando o método `start_requests()`, podemos gerar solicitações sucessivamente a partir dessas solicitações iniciais.
- **Análise:** O método `parse()`, como dito acima, é responsável pelo processamento da resposta do site e por extrair os dados necessários. Após a extração, esses dados são enviado para os Item Pipelines usando o comando `yield`.


O [[23112023141853-scrapy_exemplo_1]] abaixo demonstra a estrutura básica de uma classe `Spider`.

```python
import scrapy

class BooksSpider(scrapy.Spider): # [1]
	name = "books" # [2]

	def start_requests(self): # [3]
		url = "https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html"
		yield scrapy.Request(url, callback=self.parse)

	def parse(self, response): # [4]
		item = {} 
		product = response.css("div.product_main") 
		item["title"] = product.css("h1 ::text").get() # extract_first() em versões mais antigas
		item["category"] = response.xpath(
			"//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()"
		).get()
		item["description"] = response.xpath(
			"//div[@id='product_description']/following-sibling::p/text()"	
		).get()
		item["price"] = response.css("p.price_color ::text").get()

		yield item 

```

1. Toda Spider deve ser herdeira da classe `scrapy.Spider`. Nela são definidos alguns atributos e métodos padrão:
2. `name` : é um atributo obrigatório para toda Spider pois, como é possível haver várias spiders em um mesmo projeto, o Scrapy precisa conhecer todas pelo nome, por isso, cada spider precisa ter um nome exclusivo. 
3. `start_requests(self)`:  Este método é usado para definir o ponto de partida da Spider. A classe `Request` gera uma solicitação a partir da URL fornecida e, assim que obtém uma resposta, o método de retorno ( *callback*) , é invocado para processá-la. Neste caso, o método `parse` .
4. `parse(self, response)` :  Este é outro método padrão e o principal de uma Spider. Ele é é invocado automaticamente quando a resposta de uma solicitação é recebida. O objeto `response` contém os dados da página web recuperada. Neste método, são extraídos os dados desejados da página usando seletores [[23112023141931-css-selectors]] ou [[23112023142059-xpath-selectors]], como demonstrado no exemplo. Os dados extraídos são geralmente armazenados em um dicionário ou item personalizado, que pode ser posteriormente processado ou exportado. No exemplo, o método produz um `dict` `item` com as chaves `title`, `category`, `description` e `price`. 



A forma como as Spiders irão tratar as entradas (responses) e as saídas (spider output) podem ser personalizadas através de [[23112023141112-middlewares-de-spider]].



---
## Referencias