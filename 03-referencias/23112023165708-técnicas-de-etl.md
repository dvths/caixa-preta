---
aliases:
  - Conceitos e Técnicas de ETL
nome-do-curso: ETL e pipelines de dados com Shell, Airflow e Kafka
link: https://www.coursera.org/learn/etl-and-data-pipelines-shell-airflow-kafka/home/week/1
tipo-curso:
  - online
local:
  - Coursera
topico:
  - Técnicas de ETL
tags:
  - data_processing
  - ETL
  - revisão
sr-due: 2023-12-09
sr-interval: 3
sr-ease: 250
---
%%
cData:: `=dateformat(this.file.ctime, "dd-MM-yyyy")`
mData:: `=dateformat(this.file.mtime, "dd-MM-yyyy")`
%%

---
# Técnicas de ETL

ETL significa Extract, Transform, and Load (Extrair, Transformar e Carregar) e refere-se ao processo de curadoria de dados de várias fontes, adequando-os a um formato ou estrutura de dados unificados e carregando os dados transformados em seu novo ambiente.

## Extração

A extração de dados é o primeiro estágio do processo de ETL, no qual os dados são adquiridos de vários sistemas de origem. Os dados podem ser totalmente brutos, como dados de sensores de dispositivos IoT, ou talvez sejam dados não estruturados de documentos médicos digitalizados ou e-mails da empresa. Podem ser dados de [[24112023151014-conceito-de-processamento-de-dados-de-streeming|streaming provenientes de uma rede de mídia social ou transações de compra/venda no mercado de ações quase em tempo real]], ou podem vir de bancos de dados corporativos e [[24112023151543-conceito-de-data-warehouse|data warehouses existentes]].

## Transformação

O estágio de transformação é quando as regras e os processos são aplicados aos dados para prepará-los para o carregamento no sistema de destino, o que normalmente é feito em um ambiente de trabalho intermediário chamado de "área de preparação"(Stage Area). Aqui, os dados são limpos para garantir a confiabilidade e conformados para garantir a compatibilidade com o sistema de destino.

Muitas outras transformações podem ser aplicadas, inclusive:

- Limpeza: correção de erros ou valores ausentes
    
- Filtragem: selecionar somente o que é necessário
    
- Junção: mescla de fontes de dados diferentes
    
- Classificação: ordenar os dados para melhorar o desempenho da pesquisa
    
- Agregação: resumir dados granulares

- Normalização: conversão de dados em unidades comuns
    
- Estruturação de dados: conversão de um formato de dados em outro, como JSON, XML ou CSV em tabelas de banco de dados
    
- Engenharia de recursos: criação de KPIs para dashboards ou aprendizado de máquina
    
- Anonimização e criptografia: garantir a privacidade e a segurança
    

## Carregamento

A fase de carregamento consiste em gravar os dados transformados em um sistema de destino. O sistema pode ser tão simples quanto um arquivo separado por vírgulas (CSV), que é basicamente uma tabela de dados como uma planilha do Excel. O destino também pode ser um banco de dados, que pode ser parte de um sistema muito mais elaborado, como um data warehouse, um data mart, um data lake ou algum outro armazenamento de dados unificado e centralizado que forme a base para análise, modelagem e tomada de decisões orientadas por dados por analistas de negócios, gerentes, executivos, cientistas de dados e usuários em todos os níveis da empresa.

Na maioria dos casos, à medida que os dados são carregados em um banco de dados, as restrições definidas por seu esquema devem ser satisfeitas para que o fluxo de trabalho seja executado com êxito. O esquema, um conjunto de regras chamado de restrições de integridade, inclui regras como exclusividade, [[23112023170829-integridade-referencial|Integridade Referencial]] e campos obrigatórios. Assim, esses requisitos impostos à fase de carregamento ajudam a garantir a qualidade geral dos dados.


----
## Referências 